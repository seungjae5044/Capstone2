# Whisper 전사 설정 가이드

이 문서는 `config_whisper.json` 파일의 각 설정 항목에 대한 상세 설명을 제공합니다.

## 📋 설정 항목

### 모델 설정

#### `model_id`
- **기본값**: `"openai/whisper-large-v3-turbo"`
- **설명**: Hugging Face의 Whisper 모델 ID
- **옵션**:
  - `openai/whisper-large-v3-turbo` - 정확도와 속도의 균형 (권장)
  - `openai/whisper-large-v3` - 최고 정확도 (느림)
  - `openai/whisper-medium` - 중간 크기 (빠름)
  - `openai/whisper-small` - 가벼움 (가장 빠름, 낮은 정확도)

#### `language`
- **기본값**: `"ko"`
- **설명**: 전사할 언어 코드 (ISO 639-1)
- **옵션**: `"ko"` (한국어), `"en"` (영어), `"ja"` (일본어) 등

#### `force_device`
- **기본값**: `null`
- **설명**: 강제로 사용할 디바이스 지정
- **옵션**:
  - `null` - 자동 선택 (CUDA > MPS > CPU)
  - `"cuda"` - NVIDIA GPU
  - `"mps"` - Apple Silicon GPU
  - `"cpu"` - CPU

---

### 오디오 처리 설정

#### `audio_source_sr`
- **기본값**: `48000`
- **설명**: 마이크 입력 샘플링 레이트 (Hz)
- **권장**: 시스템 기본값 유지 (보통 48000)

#### `target_sr`
- **기본값**: `16000`
- **설명**: Whisper 모델 입력용 샘플링 레이트
- **참고**: Whisper는 16kHz에 최적화되어 있음 (변경 비권장)

#### `blocksize_seconds`
- **기본값**: `0.2`
- **설명**: 마이크에서 오디오를 읽는 블록 단위 (초)
- **조정**:
  - 낮출수록: 지연 감소, CPU 사용 증가
  - 높일수록: 지연 증가, CPU 사용 감소
- **권장 범위**: 0.1 ~ 0.5

#### `queue_maxsize`
- **기본값**: `100`
- **설명**: 오디오 청크 큐의 최대 크기
- **참고**: 큐가 가득 차면 오래된 청크 자동 버림

#### `silence_rms_threshold`
- **기본값**: `0.005`
- **설명**: 무음 판별 RMS 임계값
- **조정**:
  - 낮출수록: 더 작은 소리도 전사 (배경 소음 위험)
  - 높일수록: 큰 소리만 전사 (발화 놓칠 위험)
- **권장 범위**: 0.003 ~ 0.01

---

### 전사 처리 설정 (핵심!)

#### `chunk_length_s`
- **현재값**: `3`
- **설명**: Whisper가 한 번에 처리하는 오디오 최대 길이 (초)
- **영향**:
  - 작을수록: 빠른 전사, 짧은 컨텍스트
  - 클수록: 느린 전사, 긴 컨텍스트
- **권장 범위**: 2 ~ 5
- **최적화**: 실시간성 우선 → 3초

#### `stride_seconds`
- **현재값**: `1.0`
- **설명**: 청크 간 오버랩 시간 (초)
- **영향**:
  - 작을수록: 더 많은 새 오디오 필요 (느린 업데이트)
  - 클수록: 적은 새 오디오로 전사 가능 (빠른 업데이트)
- **제약**: `stride_seconds * 2 < chunk_length_s` (필수!)
- **권장 범위**: 0.3 ~ 1.5
- **최적화**: 1.0초 (chunk 3초의 1/3)

#### `chunk_duration`
- **현재값**: `2.0`
- **설명**: 첫 전사를 시작하기 전 오디오 버퍼 축적 시간 (초)
- **영향**:
  - 작을수록: 빠른 첫 응답, 짧은 컨텍스트
  - 클수록: 느린 첫 응답, 충분한 컨텍스트
- **권장 범위**: 1.5 ~ 5.0
- **최적화**: 2.0초 (빠른 시작)

#### `batch_size`
- **기본값**: `1`
- **설명**: Whisper 파이프라인 배치 크기
- **참고**: 실시간 전사에서는 1 유지 권장

---

### 생성 파라미터 (`generate_kwargs`)

#### `task`
- **기본값**: `"transcribe"`
- **설명**: Whisper 작업 모드
- **옵션**:
  - `"transcribe"` - 동일 언어로 전사
  - `"translate"` - 영어로 번역

#### `temperature`
- **기본값**: `0.0`
- **설명**: 생성 온도 (다양성 조절)
- **조정**:
  - `0.0` - 가장 확률 높은 토큰만 선택 (결정론적)
  - `0.5` - 약간의 다양성
  - `1.0` - 높은 다양성 (불안정할 수 있음)
- **권장**: 0.0 (정확도 우선)

#### `no_speech_threshold`
- **기본값**: `0.6`
- **설명**: 음성 없음 판정 확률 임계값
- **조정**:
  - 낮출수록: 더 많이 전사 시도 (거짓 긍정 위험)
  - 높일수록: 음성 확실할 때만 전사 (놓칠 위험)
- **권장 범위**: 0.5 ~ 0.7

#### `logprob_threshold`
- **기본값**: `-1.0`
- **설명**: 평균 로그 확률 임계값 (품질 필터)
- **조정**:
  - 높일수록: 더 엄격한 품질 요구 (거부 증가)
  - 낮을수록: 더 관대한 품질 허용
- **권장 범위**: -1.5 ~ -0.5

#### `repetition_penalty`
- **기본값**: `1.05`
- **설명**: 반복 억제 파라미터
- **조정**:
  - `1.0` - 패널티 없음
  - `1.05` - 약한 억제 (권장)
  - `1.2+` - 강한 억제 (부자연스러울 수 있음)
- **권장 범위**: 1.0 ~ 1.1

---

## 🎯 사용 시나리오별 프리셋

### 시나리오 1: 최대 실시간성 (속도 우선)
```json
{
  "chunk_length_s": 2,
  "stride_seconds": 0.5,
  "chunk_duration": 1.5,
  "silence_rms_threshold": 0.008
}
```

### 시나리오 2: 최대 정확도 (품질 우선)
```json
{
  "model_id": "openai/whisper-large-v3",
  "chunk_length_s": 5,
  "stride_seconds": 1.0,
  "chunk_duration": 3.0,
  "generate_kwargs": {
    "temperature": 0.0,
    "logprob_threshold": -0.5
  }
}
```

### 시나리오 3: 균형 잡힌 설정 (현재 설정)
```json
{
  "chunk_length_s": 3,
  "stride_seconds": 1.0,
  "chunk_duration": 2.0
}
```

---

## 🔧 설정 변경 방법

1. `config_whisper.json` 파일 편집
2. 프로그램 재시작 (자동 로드)
3. 변경 사항이 적용됨

**주의**: JSON은 주석을 지원하지 않으므로, 설정 변경 시 이 가이드 문서를 참고하세요.

---

## ⚠️ 주의사항

### 필수 제약 조건
- `stride_seconds * 2 < chunk_length_s` (안 지키면 `ValueError` 발생!)
- `chunk_duration >= chunk_length_s / 2` (권장)
- `target_sr`은 16000 유지 (Whisper 최적화)

### 성능 트레이드오프
- **chunk_length_s ↓** = 속도 ↑, 컨텍스트 ↓
- **stride_seconds ↑** = 업데이트 빈도 ↑, 중복 처리 ↑
- **chunk_duration ↓** = 첫 응답 ↑, 초기 품질 ↓

### 디버깅 팁
- 로그에서 `Whisper 파이프라인 초기화 완료` 확인
- 오류 발생 시 기본값으로 복구: `config_whisper.json` 삭제 후 재시작
- 성능 모니터링: 로그의 전사 주기 확인

---

## 📊 현재 설정 성능 분석

**현재 최적화된 설정 (config_whisper.json)**:
- ⏱️ 첫 전사 시작: ~2초
- 🔄 업데이트 주기: ~1초마다
- 📊 처리 길이: 3초 청크
- 🎯 오버랩: 33% (1초/3초)

**예상 지연시간**:
1. 사용자 발화 시작
2. 2초 대기 (버퍼 축적)
3. 첫 전사 결과 출력
4. 이후 1초마다 추가 전사

**총 지연**: 2~3초 (실시간 회의에 적합)

---

**마지막 업데이트**: 2025-10-13
**설정 버전**: v1.0 (실시간 최적화)
